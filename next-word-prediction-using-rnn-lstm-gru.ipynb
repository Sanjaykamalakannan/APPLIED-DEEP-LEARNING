{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wikipedia-api\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:13:35.479899Z","iopub.execute_input":"2025-02-18T08:13:35.480212Z","iopub.status.idle":"2025-02-18T08:13:41.856539Z","shell.execute_reply.started":"2025-02-18T08:13:35.480179Z","shell.execute_reply":"2025-02-18T08:13:41.855689Z"}},"outputs":[{"name":"stdout","text":"Collecting wikipedia-api\n  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2025.1.31)\nBuilding wheels for collected packages: wikipedia-api\n  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15384 sha256=534b8e759b98cbcf84a6ab3c899e030e35533e2f9c85e57b223b2bf090dea01c\n  Stored in directory: /root/.cache/pip/wheels/1d/f8/07/0508c38722dcd82ee355e9d85e33c9e9471d4bec0f8ae72de0\nSuccessfully built wikipedia-api\nInstalling collected packages: wikipedia-api\nSuccessfully installed wikipedia-api-0.8.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Install required packages\n!pip install wikipedia-api \n\nimport wikipediaapi\nimport numpy as np\nimport nltk\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, GRU, Dense\nimport random\n\n# Download NLTK resources\nnltk.download('punkt')\n\n# Function to fetch real-time text from Wikipedia\ndef fetch_wikipedia_text(topic=\"Artificial Intelligence\"):\n    wiki = wikipediaapi.Wikipedia(language='en', user_agent='My_Wikipedia_App')  \n    page = wiki.page(topic)\n    if page.exists():\n        return page.text[:5000]  # Fetch first 5000 characters\n    return \"No content found.\"\n\n# Preprocessing text\ndef preprocess_text(text):\n    tokens = nltk.word_tokenize(text.lower())  # Tokenize and lowercase\n    return \" \".join(tokens)\n\n# Get real-time text\ntopic = \"Machine Learning\"\ntext_data = fetch_wikipedia_text(topic)\ncleaned_text = preprocess_text(text_data)\n\n# Tokenization\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([cleaned_text])\ntotal_words = len(tokenizer.word_index) + 1\n\n# Create input-output sequences\ninput_sequences = []\nwords = cleaned_text.split()\nfor i in range(1, len(words)):\n    n_gram_sequence = words[:i+1]\n    encoded_seq = tokenizer.texts_to_sequences([\" \".join(n_gram_sequence)])[0]\n    input_sequences.append(encoded_seq)\n\n# Pad sequences\nmax_length = max(len(seq) for seq in input_sequences)\ninput_sequences = pad_sequences(input_sequences, maxlen=max_length, padding='pre')\n\n# Split input (X) and output (y)\nX, y = input_sequences[:, :-1], input_sequences[:, -1]\ny = tf.keras.utils.to_categorical(y, num_classes=total_words)\n\n# Define LSTM Model\ndef build_lstm_model():\n    with tf.device('/GPU:0'):  # Ensure running on GPU\n        model = Sequential([\n            Embedding(total_words, 50),\n            LSTM(128, return_sequences=True),\n            LSTM(64),\n            Dense(total_words, activation='softmax')\n        ])\n        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n# Define GRU Model\ndef build_gru_model():\n    with tf.device('/GPU:0'):  # Ensure running on GPU\n        model = Sequential([\n            Embedding(total_words, 50),\n            GRU(128, return_sequences=True),\n            GRU(64),\n            Dense(total_words, activation='softmax')\n        ])\n        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n# Train LSTM Model\nlstm_model = build_lstm_model()\nlstm_model.fit(X, y, epochs=6, verbose=1)\n\n# Train GRU Model\ngru_model = build_gru_model()\ngru_model.fit(X, y, epochs=6, verbose=1)\n\n# Improved next-word prediction function\ndef predict_next_words(model, seed_text, num_words=3):\n    output_text = seed_text\n    \n    for _ in range(num_words):\n        token_list = tokenizer.texts_to_sequences([output_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_length-1, padding='pre')\n        \n        predicted_probs = model.predict(token_list, verbose=0)  # Get probabilities\n        predicted_index = np.argmax(predicted_probs, axis=-1)[0]  # Get most probable index\n        \n        output_word = None\n        for word, index in tokenizer.word_index.items():\n            if index == predicted_index:\n                output_word = word\n                break\n        \n        if output_word is None:  # If no valid word is found, stop\n            break\n        \n        output_text += \" \" + output_word\n\n    return output_text\n\n# Test predictions\nseed_sentence = \"Machine learning is\"\nlstm_prediction = predict_next_words(lstm_model, seed_sentence, 3)\ngru_prediction = predict_next_words(gru_model, seed_sentence, 3)\n\nprint(\"\\nLSTM Prediction:\", lstm_prediction)\nprint(\"GRU Prediction:\", gru_prediction)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:18:25.700754Z","iopub.execute_input":"2025-02-18T08:18:25.701127Z","iopub.status.idle":"2025-02-18T08:18:55.618008Z","shell.execute_reply.started":"2025-02-18T08:18:25.701103Z","shell.execute_reply":"2025-02-18T08:18:55.617235Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.10/dist-packages (0.8.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2025.1.31)\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nEpoch 1/6\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 67ms/step - accuracy: 0.0115 - loss: 5.8943\nEpoch 2/6\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.0367 - loss: 5.5776\nEpoch 3/6\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.0400 - loss: 5.4278\nEpoch 4/6\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.0247 - loss: 5.4651\nEpoch 5/6\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.0406 - loss: 5.3549\nEpoch 6/6\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.0487 - loss: 5.3266\nEpoch 1/6\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - accuracy: 0.0140 - loss: 5.9061\nEpoch 2/6\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.0290 - loss: 5.5870\nEpoch 3/6\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.0403 - loss: 5.4325\nEpoch 4/6\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.0730 - loss: 5.3404\nEpoch 5/6\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.0489 - loss: 5.3427\nEpoch 6/6\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.0475 - loss: 5.3399\n\nLSTM Prediction: Machine learning is learning learning learning\nGRU Prediction: Machine learning is learning the learning\n","output_type":"stream"}],"execution_count":4}]}