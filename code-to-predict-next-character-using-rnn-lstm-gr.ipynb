{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required packages\n!pip install wikipedia-api ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:26:11.037351Z","iopub.execute_input":"2025-02-18T08:26:11.037616Z","iopub.status.idle":"2025-02-18T08:26:17.597680Z","shell.execute_reply.started":"2025-02-18T08:26:11.037579Z","shell.execute_reply":"2025-02-18T08:26:17.596835Z"}},"outputs":[{"name":"stdout","text":"Collecting wikipedia-api\n  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2025.1.31)\nBuilding wheels for collected packages: wikipedia-api\n  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15384 sha256=10182e4d3c8d68ffaf18f583b76c297da12c18182a7114ca073b273a30a367f7\n  Stored in directory: /root/.cache/pip/wheels/1d/f8/07/0508c38722dcd82ee355e9d85e33c9e9471d4bec0f8ae72de0\nSuccessfully built wikipedia-api\nInstalling collected packages: wikipedia-api\nSuccessfully installed wikipedia-api-0.8.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport wikipediaapi\nimport nltk\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Download NLTK resources\nnltk.download('punkt')\n\n# Function to fetch real-time text from Wikipedia\ndef fetch_wikipedia_text(topic=\"Artificial Intelligence\"):\n    wiki = wikipediaapi.Wikipedia(language='en', user_agent='My_Wikipedia_App')  \n    page = wiki.page(topic)\n    if page.exists():\n        return page.text[:5000]  # Fetch first 5000 characters\n    return \"No content found.\"\n\n# Preprocessing text\ndef preprocess_text(text):\n    return \"\".join([char.lower() for char in text if char.isalnum() or char.isspace()])\n\n# Fetch and clean Wikipedia text\ntopic = \"Machine Learning\"\ntext_data = fetch_wikipedia_text(topic)\ncleaned_text = preprocess_text(text_data)\n\n# Create character-level vocabulary\nchars = sorted(set(cleaned_text))  # Unique characters\nchar_to_index = {char: i for i, char in enumerate(chars)}\nindex_to_char = {i: char for char, i in char_to_index.items()}\ntotal_chars = len(chars)\n\n# Prepare input-output sequences (sliding window approach)\nseq_length = 4  # Sequence length increased for better learning\ninput_sequences, output_chars = [], []\n\nfor i in range(len(cleaned_text) - seq_length):\n    input_seq = cleaned_text[i:i + seq_length]\n    output_char = cleaned_text[i + seq_length]\n    input_sequences.append([char_to_index[c] for c in input_seq])\n    output_chars.append(char_to_index[output_char])\n\n# Convert to NumPy arrays\nX = np.array(input_sequences)\ny = tf.keras.utils.to_categorical(output_chars, num_classes=total_chars)\n\n# Define RNN Model\ndef build_rnn_model():\n    with tf.device('/GPU:0'):\n        model = Sequential([\n            Embedding(total_chars, 50),  # Removed input_length\n            SimpleRNN(128, return_sequences=True),\n            SimpleRNN(64),\n            Dense(total_chars, activation='softmax')\n        ])\n        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n# Define LSTM Model\ndef build_lstm_model():\n    with tf.device('/GPU:0'):\n        model = Sequential([\n            Embedding(total_chars, 50),  # Removed input_length\n            LSTM(128, return_sequences=True),\n            LSTM(64),\n            Dense(total_chars, activation='softmax')\n        ])\n        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n# Define GRU Model\ndef build_gru_model():\n    with tf.device('/GPU:0'):\n        model = Sequential([\n            Embedding(total_chars, 50),  # Removed input_length\n            GRU(128, return_sequences=True),\n            GRU(64),\n            Dense(total_chars, activation='softmax')\n        ])\n        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n# Train models with 10 epochs for better accuracy\nrnn_model = build_rnn_model()\nlstm_model = build_lstm_model()\ngru_model = build_gru_model()\n\nrnn_model.fit(X, y, epochs=10, verbose=1)\nlstm_model.fit(X, y, epochs=10, verbose=1)\ngru_model.fit(X, y, epochs=10, verbose=1)\n\n# Next-character prediction function\ndef predict_next_chars(model, seed_text, num_chars=3):\n    for _ in range(num_chars):\n        token_list = [char_to_index[c] for c in seed_text if c in char_to_index]\n        token_list = pad_sequences([token_list], maxlen=seq_length, padding='pre')  # Ensure correct length\n        \n        predicted_index = np.argmax(model.predict(token_list), axis=-1)[0]\n        output_char = index_to_char.get(predicted_index, \"?\")  # Handle unknown characters\n        \n        seed_text += output_char\n    return seed_text\n\n# **Testing character prediction**\nword = \"go\"\npredicted_rnn = predict_next_chars(rnn_model, word, 3)\npredicted_lstm = predict_next_chars(lstm_model, word, 3)\npredicted_gru = predict_next_chars(gru_model, word, 3)\n\nprint(\"\\nRNN Prediction:\", predicted_rnn)\nprint(\"LSTM Prediction:\", predicted_lstm)\nprint(\"GRU Prediction:\", predicted_gru)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:31:17.446107Z","iopub.execute_input":"2025-02-18T08:31:17.446487Z","iopub.status.idle":"2025-02-18T08:31:45.660482Z","shell.execute_reply.started":"2025-02-18T08:31:17.446464Z","shell.execute_reply":"2025-02-18T08:31:45.659596Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nEpoch 1/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.1404 - loss: 3.1509\nEpoch 2/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2369 - loss: 2.6678\nEpoch 3/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3190 - loss: 2.3847\nEpoch 4/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3521 - loss: 2.2216\nEpoch 5/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3875 - loss: 2.1400\nEpoch 6/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4033 - loss: 2.0590\nEpoch 7/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4146 - loss: 2.0154\nEpoch 8/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4403 - loss: 1.9048\nEpoch 9/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4630 - loss: 1.8646\nEpoch 10/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4769 - loss: 1.7688\nEpoch 1/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1438 - loss: 3.2520\nEpoch 2/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1518 - loss: 2.9271\nEpoch 3/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1622 - loss: 2.8931\nEpoch 4/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1876 - loss: 2.7803\nEpoch 5/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2622 - loss: 2.5628\nEpoch 6/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3109 - loss: 2.3582\nEpoch 7/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3506 - loss: 2.2425\nEpoch 8/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3712 - loss: 2.1378\nEpoch 9/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3867 - loss: 2.0817\nEpoch 10/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4142 - loss: 1.9837\nEpoch 1/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1410 - loss: 3.2332\nEpoch 2/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.1769 - loss: 2.8100\nEpoch 3/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.2803 - loss: 2.4466\nEpoch 4/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3304 - loss: 2.2597\nEpoch 5/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3631 - loss: 2.1303\nEpoch 6/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4020 - loss: 2.0347\nEpoch 7/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4296 - loss: 1.9399\nEpoch 8/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4591 - loss: 1.8254\nEpoch 9/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4842 - loss: 1.7399\nEpoch 10/10\n\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5228 - loss: 1.6274\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\nRNN Prediction: gonis\nLSTM Prediction: goter\nGRU Prediction: goper\n","output_type":"stream"}],"execution_count":4}]}